{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration\n",
    "\n",
    "In this notebook I am going to explore the data and processes of the first project of the NanoDegree Data Engineering Program.\n",
    "\n",
    "- 'Walk' the directory with Path\n",
    "- Retrieve all JSON files\n",
    "- Load them into Pandas \n",
    "- Explore and clean\n",
    "- Ready for insert -> NOTE: watch out with insertion in the database, something like auto increment oid should be ON \n",
    "- Create a SQL database (SQLlite for training purposes?)\n",
    "- Insert the transformed and cleaned data\n",
    "- Bonus: Logging (!) -> try to keep it simple but logging is essential for these tasks -> especially for Exception statements etc.\n",
    "\n",
    "Make sure to write clean robust code, add sensible checks -> assert. Eventually this will become your etl.ipynb \n",
    "\n",
    "----------------------------\n",
    "\n",
    "After the first exploration, there appears to be a recurring process:\n",
    "- Collect the data for a particular table\n",
    "- Assert that the data is correct\n",
    "- Insert the data into the specified table\n",
    "- Verify the results\n",
    "\n",
    "There is probably a lot of functionality we can re-use for each table.\n",
    "\n",
    "#### Find JSON files and return the directories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import user, password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path_list(file_path: str, extension: str = '.json') -> list:\n",
    "    \"\"\"Returns a list of Paths of all the files with the extension. All subdirectories of file_path are included.\n",
    "    \n",
    "    Example\n",
    "        from pathlib import Path\n",
    "        \n",
    "        data_path = Path('.') / 'data'\n",
    "        csv_path_list = create_path_list(data_path, '.csv')     \n",
    "    \"\"\"\n",
    "    return_list = [x for x in file_path.glob(f\"**/*{extension}\")]\n",
    "    print(f\"{file_path} contains {len(return_list)} {extension} files.\")    \n",
    "    \n",
    "    return return_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('.') / 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_list = create_path_list(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process song data\n",
    "\n",
    "From this data 2 tables are created:\n",
    "- songs - songs in music database -> song_id, title, artist_id, year, duration\n",
    "- artists - artists in music database -> artist_id, name, location, latitude, longitude\n",
    "\n",
    "Create a list of tuples for a direct insert into the Postgres table. Validate each row, concatenate to a Dataframe, transform to a list of tuples and return.\n",
    "\n",
    "https://realpython.com/python-exceptions/#the-assertionerror-exception\n",
    "\n",
    "Observations:\n",
    "- Songs: year contains 0 values\n",
    "\n",
    "#### Songs table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_path_list = create_path_list(data_path / 'song_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_columns = sorted(['title', 'song_id', 'year', 'duration'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_assertions(df: pd.DataFrame, target_columns: list) -> None:\n",
    "    \"\"\"Assert statements to make sure the retrieved data is valid and clean before insertion into the Postgres table.\"\"\"     \n",
    "    df_cols = df.columns.str.lower() \n",
    "    found_cols = [x for x in df_cols if x in target_columns]\n",
    "    \n",
    "    assert sorted(found_cols) == sorted(target_columns), f\"The columns do not match.\"\n",
    "    assert df[target_columns].isnull().values.any() == False, f\"Missing values in not nullable target columns.\"\n",
    "    # assert data types of each column   \n",
    "    # assert any constraint important to the Postgres Table \n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df = pd.DataFrame(columns=song_columns)\n",
    "\n",
    "for idx, file in enumerate(song_path_list):\n",
    "    temp_df = pd.read_json(file, lines=True)    \n",
    "    try:\n",
    "        df_assertions(temp_df, song_columns)\n",
    "    except AssertionError as error:\n",
    "        print(f\"Error @ file {idx} {file}: {error} NOTE: this file will not be inserted.\")\n",
    "    else:\n",
    "        song_df = song_df.append(temp_df[song_columns], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_table_data = list(song_df.to_records(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Artists table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df[['artist_name', 'artist_location', 'artist_latitude', 'artist_longitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_columns = ['artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_df = pd.DataFrame(columns=artist_columns)\n",
    "\n",
    "for idx, file in enumerate(song_path_list):\n",
    "    temp_df = pd.read_json(file, lines=True)    \n",
    "    artist_df = artist_df.append(temp_df[artist_columns], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_df.replace({'': None, np.nan: None})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging data\n",
    "\n",
    "There are 2 tables we want to extract from logging data\n",
    "- users\n",
    "- time\n",
    "\n",
    "Challenge:\n",
    "- users needs to be filtered on ['auth']=='Logged In']\n",
    "- time needs to be filterd on ['page']=='NextSong'\n",
    "- Ideally, we do not want to load the data twice since that would cause A lot of overhead...\n",
    "\n",
    "Create a function which takes temp_df as input, and returns a temp_users and temp_time df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path_list = create_path_list(data_path / 'log_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = pd.read_json(log_path_list[0], lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expands_dfs(temp_df):\n",
    "    \"\"\"Returns 2 dataframes which can be used for the users and time tables in Postgres.\"\"\"\n",
    "    users_columns = ['userId', 'firstName', 'lastName', 'gender', 'level']\n",
    "    time_columns = ['ts']\n",
    "    \n",
    "    song_df = temp_df[temp_df['auth']=='Logged In']\n",
    "    time_df = temp_df[temp_df['page']=='NextSong']\n",
    "    \n",
    "    return (song_df[users_columns], time_df[time_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expands_dfs(log_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df = pd.DataFrame()\n",
    "time_df = pd.DataFrame()\n",
    "\n",
    "for idx, file in enumerate(log_path_list):\n",
    "    temp_df = pd.read_json(file, lines=True)   \n",
    "    \n",
    "    temp_song, temp_time = expands_dfs(temp_df)\n",
    "    \n",
    "    song_df = song_df.append(temp_song, ignore_index=True)\n",
    "    time_df = time_df.append(temp_time, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expand_ms(time_df['ts']).drop_duplicates(subset='start_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_ms(ms_series: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"Expands a Pandas series with milliseconds with several datetime attributes.\"\"\"\n",
    "    df = pd.DataFrame({'start_time': pd.to_datetime(ms_series, unit='ms')})\n",
    "    \n",
    "    df['hour'] = df['start_time'].dt.hour\n",
    "    df['day'] = df['start_time'].dt.day\n",
    "    df['day'] = df['start_time'].dt.isocalendar().week  \n",
    "    df['month'] = df['start_time'].dt.month\n",
    "    df['year'] = df['start_time'].dt.year\n",
    "    df['weekday'] = df['start_time'].dt.weekday\n",
    "\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time in time_df:\n",
    "    pd.DataFrame({'start_time': pd.to_datetime(time, unit='ms')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(time_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path_list = create_path_list(data_path / 'log_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = pd.read_json(log_path_list[0], lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df['page'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_songs = log_df[log_df['page']=='NextSong']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Users -> [user_id, first_name, last_name, gender, level]\n",
    "Levels feels like something we should update... hence the hints on the cheat sheet :D -> take time into account, the files are in chronological order\n",
    "\n",
    "- Per file we should only keep one row per userid\n",
    "- If the userid already exists, check if we can update gender and / or level, else skip\n",
    "- make sure to read the files in the correct order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_dtype_dict = {'userId': int,\n",
    "                    'firstName': str, \n",
    "                    'lastName': str,\n",
    "                    'gender': str,\n",
    "                    'level': str} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_columns = ['userId', 'firstName', 'lastName', 'gender', 'level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path_list = create_path_list(data_path / 'log_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make sure the data files are in correct chronological order + make sure the data is appended in chronological order!! this way we can keep track whom changes their level accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(log_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_log_assertions(df: pd.DataFrame, target_columns: list, not_nullable_columns: list = None) -> None:\n",
    "    \"\"\"Assert statements to make sure the retrieved data is valid and clean before insertion into the Postgres table.\"\"\"     \n",
    "    low_df_columns = [x.lower() for x in df.columns]\n",
    "    low_target_columns = [x.lower() for x in target_columns] \n",
    "    \n",
    "    found_cols = [x for x in low_df_columns if x in low_target_columns]    \n",
    "    assert sorted(found_cols) == sorted(low_target_columns), f\"The columns do not match.\"\n",
    "    \n",
    "    if not_nullable_columns:\n",
    "        assert df[not_nullable_columns].isnull().values.any() == False, f\"Missing values in not nullable target columns.\"\n",
    "    else:\n",
    "        assert df[target_columns].isnull().values.any() == False, f\"Missing values in the target columns, if allowed please specify these columns.\"\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_log_insert_list(file_path_list: list, insert_columns: list, primary_keys: list,\n",
    "                           dtype_dict: dict, not_nullable_columns: list = None) -> list:\n",
    "    \"\"\"Takes a raw file_path list as input, performs several validation checks, and returns a list of tuples\n",
    "    ready for insertion in Postgres.\"\"\"\n",
    "    target_df = pd.DataFrame(columns=insert_columns)\n",
    "\n",
    "    for idx, file in enumerate(file_path_list):\n",
    "        temp_df = pd.read_json(file, lines=True)   \n",
    "\n",
    "        try:\n",
    "            df_log_assertions(temp_df, insert_columns, not_nullable_columns)\n",
    "        except AssertionError as error:\n",
    "            print(f\"AssertionError @ file {idx} {file}: {error} NOTE: this file will not be inserted.\")\n",
    "        else:\n",
    "            try:\n",
    "                # we do not want to store non logged in users\n",
    "                temp_df = temp_df[temp_df['auth']=='Logged In']\n",
    "                temp_df[insert_columns] = temp_df[insert_columns].astype(dtype_dict)\n",
    "            except ValueError as error:\n",
    "                print(f\"ValueError @ file {idx} {file}: {error} NOTE: this file will not be inserted\")\n",
    "            else:\n",
    "                target_df = target_df.append(temp_df[insert_columns], ignore_index=True)\n",
    "                \n",
    "    insert_df = target_df.drop_duplicates(subset=primary_keys)\n",
    "    print(f\"There were {target_df.shape[0]-insert_df.shape[0]} duplicate primary keys removed from the insert dataframe\")\n",
    "                \n",
    "    if not_nullable_columns:\n",
    "        insert_df = insert_df.replace({'': None, np.nan: None})  # Postgres does not recognize '' or np.nan as NULL\n",
    "    \n",
    "    # This list comprehension converts the numpy dtypes to standard python dtypes which are necessary for Postgres\n",
    "    return (insert_df, [tuple(row) for row in insert_df.itertuples(index=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df, log_tuple_list = create_log_insert_list(file_path_list=log_path_list,\n",
    "                                                insert_columns=users_columns,\n",
    "                                                primary_keys=['userId', 'gender', 'level'],\n",
    "                                                dtype_dict=users_dtype_dict,\n",
    "                                                not_nullable_columns=['userId', 'level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _df_assertions(df: pd.DataFrame, target_columns: list, not_nullable_columns: list = None) -> None:\n",
    "    \"\"\"Assert statements to make sure the retrieved data is valid and clean before insertion into the Postgres table.\"\"\"     \n",
    "    low_df_columns = [x.lower() for x in df.columns]\n",
    "    low_target_columns = [x.lower() for x in target_columns] \n",
    "    \n",
    "    found_cols = [x for x in low_df_columns if x in low_target_columns]    \n",
    "    assert sorted(found_cols) == sorted(low_target_columns), f\"The columns do not match.\"\n",
    "    \n",
    "    if not_nullable_columns:\n",
    "        assert df[not_nullable_columns].isnull().values.any() == False, f\"Missing values in not nullable target columns.\"\n",
    "    else:\n",
    "        assert df[target_columns].isnull().values.any() == False, f\"Missing values in the target columns, if allowed please specify these columns.\"\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _expand_ms(ms_series: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"Expands a Pandas series of milliseconds with several datetime attributes.\"\"\"\n",
    "    df = pd.DataFrame({'start_time': pd.to_datetime(ms_series, unit='ms')})\n",
    "    \n",
    "    df['hour'] = df['start_time'].dt.hour\n",
    "    df['day'] = df['start_time'].dt.day\n",
    "    df['day'] = df['start_time'].dt.isocalendar().week  \n",
    "    df['month'] = df['start_time'].dt.month\n",
    "    df['year'] = df['start_time'].dt.year\n",
    "    df['weekday'] = df['start_time'].dt.weekday\n",
    "\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_log_insert_lists(file_path_list: list, insert_columns: list, primary_keys: list,\n",
    "                            dtype_dict: dict, not_nullable_columns: list = None) -> list:\n",
    "    \"\"\"Takes a raw file_path list as input, performs several validation checks, and returns a list of tuples\n",
    "    ready for insertion in Postgres.\"\"\"\n",
    "    target_df = pd.DataFrame(columns=insert_columns)\n",
    "\n",
    "    for idx, file in enumerate(file_path_list):\n",
    "        temp_df = pd.read_json(file, lines=True)   \n",
    "\n",
    "        try:\n",
    "            _df_log_assertions(temp_df, insert_columns, not_nullable_columns)\n",
    "        except AssertionError as error:\n",
    "            print(f\"AssertionError @ file {idx} {file}: {error} NOTE: this file will not be inserted.\")\n",
    "        else:\n",
    "            try:\n",
    "                # we do not want to store non logged in users\n",
    "                temp_df = temp_df[temp_df['auth']=='Logged In']\n",
    "                temp_df[insert_columns] = temp_df[insert_columns].astype(dtype_dict)\n",
    "            except ValueError as error:\n",
    "                print(f\"ValueError @ file {idx} {file}: {error} NOTE: this file will not be inserted\")\n",
    "            else:\n",
    "                target_df = target_df.append(temp_df[insert_columns], ignore_index=True)\n",
    "                \n",
    "    insert_users_df = target_df.drop_duplicates(subset=primary_keys)\n",
    "    print(f\"There were {target_df.shape[0]-insert_users_df.shape[0]} duplicate primary keys removed from the insert dataframe\")\n",
    "    \n",
    "    insert_time_df = _expand_ms(target_df['ts'])\n",
    "                \n",
    "    if not_nullable_columns:\n",
    "        insert_users_df = insert_users_df.replace({'': None, np.nan: None})  # Postgres does not recognize '' or np.nan as NULL\n",
    "    \n",
    "    # The list comprehension converts the numpy dtypes to standard python dtypes which are necessary for Postgres\n",
    "    return ([tuple(row) for row in insert_users_df.itertuples(index=False)], [tuple(row) for row in insert_time_df.itertuples(index=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dtype_dict = {'userId': int,\n",
    "                  'firstName': str, \n",
    "                  'lastName': str,\n",
    "                  'gender': str,\n",
    "                  'level': str,\n",
    "                  'ts': int} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_columns = ['userId', 'firstName', 'lastName', 'gender', 'level', 'ts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_table_data, time_table_data = create_log_insert_lists(file_path_list=log_path_list,\n",
    "                                                            insert_columns=log_columns,\n",
    "                                                            primary_keys=['userId', 'gender', 'level'],\n",
    "                                                            dtype_dict=log_dtype_dict,\n",
    "                                                            not_nullable_columns=['userId', 'level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(time_table_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time table\n",
    "- start_time, hour, day, week, month, year, weekday\n",
    "- ts (timestamp) of records in log data with page=NextSong\n",
    "- Convert back to ms when joining on the other tables..if necessary..\n",
    "\n",
    "#### Extract Data for Time Table\n",
    "- Filter records by `NextSong` action\n",
    "- Convert the `ts` timestamp column to datetime\n",
    "  - Hint: the current timestamp is in milliseconds\n",
    "- Extract the timestamp, hour, day, week of year, month, year, and weekday from the `ts` column and set `time_data` to a list containing these values in order\n",
    "  - Hint: use pandas' [`dt` attribute](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.html) to access easily datetimelike properties.\n",
    "- Specify labels for these columns and set to `column_labels`\n",
    "- Create a dataframe, `time_df,` containing the time data for this file by combining `column_labels` and `time_data` into a dictionary and converting this into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path_list = create_path_list(data_path / 'log_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = pd.read_json(log_path_list[0], lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df['ts'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_ms(ms_series: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"Expands a Pandas series with milliseconds with several datetime attributes.\"\"\"\n",
    "    df = pd.DataFrame({'start_time': pd.to_datetime(ms_series, unit='ms')})\n",
    "    \n",
    "    df['hour'] = df['start_time'].dt.hour\n",
    "    df['day'] = df['start_time'].dt.day\n",
    "    df['day'] = df['start_time'].dt.isocalendar().week  \n",
    "    df['month'] = df['start_time'].dt.month\n",
    "    df['year'] = df['start_time'].dt.year\n",
    "    df['weekday'] = df['start_time'].dt.weekday\n",
    "\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expand_ms(log_df['ts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expand_ms(log_df['ts']).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(expand_ms(log_df['ts']).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Songplays\n",
    "\n",
    "#### Extract Data and Songplays Table\n",
    "This one is a little more complicated since information from the songs table, artists table, and original log file are all needed for the `songplays` table. Since the log file does not specify an ID for either the song or the artist, you'll need to get the song ID and artist ID by querying the songs and artists tables to find matches based on song title, artist name, and song duration time.\n",
    "- Implement the `song_select` query in `sql_queries.py` to find the song ID and artist ID based on the title, artist name, and duration of a song.\n",
    "- Select the timestamp, user ID, level, song ID, artist ID, session ID, location, and user agent and set to `songplay_data`\n",
    "\n",
    "#### Insert Records into Songplays Table\n",
    "- Implement the `songplay_table_insert` query and run the cell below to insert records for the songplay actions in this log file into the `songplays` table. Remember to run `create_tables.py` before running the cell below to ensure you've created/resetted the `songplays` table in the sparkify database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
